{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file-handling\n",
    "import os \n",
    "\n",
    "# user status updates\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Latex\n",
    "from datetime import datetime\n",
    "\n",
    "# the holy trinity of python data science\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "from scipy import sparse, linalg, fft\n",
    "from scipy.linalg import expm, sinm, cosm\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# parallelization, memory management\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit, njit, prange\n",
    "import copy\n",
    "\n",
    "# itertools\n",
    "import itertools\n",
    "\n",
    "############ Macros ###############\n",
    "NP_RANDOM_SEED = 0\n",
    "TORCH_RANDOM_SEED = 0\n",
    "np.random.seed(NP_RANDOM_SEED)\n",
    "torch.manual_seed(TORCH_RANDOM_SEED)\n",
    "\n",
    "# Physical constants\n",
    "N = 10 # number of fermions\n",
    "J = 100 # ~\"energy scale\" of couplings\n",
    "Q_COUPLING = 3 # order of coupling, don't want to use the letter 'Q' because that denotes the supercharge\n",
    "N_DIM = 2**N # Hilbert space dimension \n",
    "FIRST_NONZERO = 0.15 # first nonzero eigenvalue, for our intents and purposes\n",
    "\n",
    "# Computer stuff\n",
    "N_SAMPLES = 100 # number of samples to generate\n",
    "N_JOBS = 20 # number of jobs to run in parallel\n",
    "\n",
    "# Model stuff\n",
    "LR = 1e-5 # learning rate\n",
    "BATCH_SIZE = 100 # batch size for training the neural network\n",
    "N_EPOCHS = 1 # number of epochs to train the neural network\n",
    "R_TRAIN = 0.5 # desired ratio of zero eigenvalues to total eigenvalues IN TRAINING SET\n",
    "RUN_COUNTER = 0 # counter for how many times we've trained the model for N_EPOCHS\n",
    "RUNS_DICT = {} # keep track of epochs and learning rate used for training\n",
    "TEST_RANDOM = True # whether to train and test the randomly-trained model\n",
    "\n",
    "# Directories\n",
    "N2_SUSY_DIR = os.path.join(\"Excel\", \"N2_SUSY_SYK\")\n",
    "RESULT_DIR = os.path.join(N2_SUSY_DIR, \"Simulated Hamiltonians\", f\"N{N}_J{J}\")\n",
    "MLP_DIR = os.path.join(N2_SUSY_DIR, \"MLP\", f\"N{N}_J{J}\")\n",
    "os.makedirs(MLP_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##       1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IV = N_SAMPLES*N_DIM\n",
    "ivals_all = np.zeros(shape=(N_IV,), dtype=np.float64)\n",
    "ivecs_all = np.zeros(shape=(N_IV, N_DIM), dtype=np.complex128)\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    ivals_i = np.load(os.path.join(RESULT_DIR, f\"ivals_{i}.npy\"))\n",
    "    ivecs_i = np.load(os.path.join(RESULT_DIR, f\"ivecs_{i}.npy\"))\n",
    "    ivals_all[i*N_DIM:(i+1)*N_DIM] = ivals_i\n",
    "    ivecs_all[i*N_DIM:(i+1)*N_DIM,:] = ivecs_i\n",
    "\n",
    "labels_all = (ivals_all>=FIRST_NONZERO).astype(int) # zero-energy eigenvectors labeled with 0, finite-energy eigenvectors labeled with 1\n",
    "print(f\"ivals_all.shape: {ivals_all.shape}\")\n",
    "print(f\"ivecs_all.shape: {ivecs_all.shape}\")\n",
    "print(f\"labels_all.shape: {labels_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convert (1xN_DIM) complex into (1x2*N_DIM) real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch doesn't currently have handling for complex numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivecs_all_new = np.zeros(shape=(N_IV, 2*N_DIM), dtype=np.float64)\n",
    "\n",
    "for i in range(N_IV):\n",
    "    ivecs_all_i = ivecs_all[i]\n",
    "    ivecs_all_new[i] = np.concatenate((ivecs_all_i.real, ivecs_all_i.imag), axis=0)\n",
    "\n",
    "ivecs_all = ivecs_all_new\n",
    "del ivecs_all_new\n",
    "\n",
    "print(f\"ivecs_all.shape = {ivecs_all.shape}\")\n",
    "N_DIM *= 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train-test-validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, random shuffle\n",
    "shuffler = np.random.permutation(N_IV)\n",
    "ivecs_all = ivecs_all[shuffler]\n",
    "labels_all = labels_all[shuffler]\n",
    "\n",
    "p_train = 0.7\n",
    "n_train = int(p_train*N_IV)\n",
    "X_train = ivecs_all[:n_train]\n",
    "y_train = labels_all[:n_train]\n",
    "\n",
    "p_val = 0.2\n",
    "n_val = int(p_val*N_IV)\n",
    "X_val = ivecs_all[n_train:n_train+n_val]\n",
    "y_val = labels_all[n_train:n_train+n_val]\n",
    "\n",
    "p_test = 0.1\n",
    "n_test = int(p_test*N_IV)\n",
    "X_test = ivecs_all[n_train+n_val:]\n",
    "y_test = labels_all[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 Balance training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_{train} = $ desired proportion of zero-energy eigenvectors\n",
    "\n",
    "$N_0 =$ number of zero-energy eigenvectors\n",
    "\n",
    "$N_f =$ number of finite-energy eigenvectors\n",
    "\n",
    "$N_{add}$ number of zero-energy eigenvectors we must add to achieve $r_{want}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To find $N_{add}$, we start off with an identity:\n",
    "\n",
    "$r_{train} \\equiv \\frac{N_0+N_{add}}{N_0+N_f+N_{add}} $\n",
    "\n",
    "$N_0+N_{add} = r_{train} (N_0+N_f+N_{add})$\n",
    "\n",
    "$N_0+N_{add} = r_{train}*N_0+r_{train}*N_f+r_{train}*N_{add}$\n",
    "\n",
    "$N_{add}(1-r_{train}) = N_0(r_{train}-1)+r_{train}*N_f$\n",
    "\n",
    "\n",
    "And finally,\n",
    "\n",
    "$N_{add} = \\frac{N_0(r_{train}-1)+N_f*r_{train}}{1-r_{train}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_0 = sum(y_train==0)\n",
    "N_f = sum(y_train==1)\n",
    "N_add = (N_0*(R_TRAIN-1)+N_f*R_TRAIN)/(1-R_TRAIN)\n",
    "\n",
    "X_train_0 = X_train[y_train==0]\n",
    "X_train_0_clone_idx = np.random.choice(range(X_train_0.shape[0]), size=int(N_add), replace=True)\n",
    "X_train_0_clone = copy.deepcopy(X_train_0[X_train_0_clone_idx])\n",
    "y_train_0_clone = np.zeros(shape=(X_train_0_clone.shape[0],), dtype=np.int32)\n",
    "\n",
    "X_train = np.concatenate((X_train, X_train_0_clone), axis=0)\n",
    "y_train = np.concatenate((y_train, y_train_0_clone), axis=0)\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"N_0: {sum(y_train==0)/len(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Convert to Torch tensors that MLP can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().cuda()\n",
    "y_train = torch.from_numpy(y_train).float().cuda()\n",
    "\n",
    "X_val = torch.from_numpy(X_val).float().cuda()\n",
    "y_val = torch.from_numpy(y_val).float().cuda()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().cuda()\n",
    "y_test = torch.from_numpy(y_test).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden1_dim, hidden2_dim, hidden3_dim, hidden4_dim):\n",
    "        \n",
    "        super(BinaryClassifier, self).__init__()\n",
    "        \n",
    "        self.layer1 = nn.Linear(input_dim, hidden1_dim)\n",
    "        self.layer2 = nn.Linear(hidden1_dim, hidden2_dim)\n",
    "        self.layer3 = nn.Linear(hidden2_dim, hidden3_dim)\n",
    "        self.layer4 = nn.Linear(hidden3_dim, hidden4_dim)\n",
    "        self.layer5 = nn.Linear(hidden4_dim, output_dim)\n",
    "\n",
    "    # Function to pass data forward through network.\n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.layer1(x))\n",
    "        out2 = F.relu(self.layer2(out1))\n",
    "        out3 = F.relu(self.layer3(out2))\n",
    "        out4 = F.relu(self.layer4(out3))\n",
    "        out5 = F.sigmoid(self.layer5(out4))\n",
    "        \n",
    "        return out5\n",
    "\n",
    "# Initialize model and random model \n",
    "model = BinaryClassifier(N_DIM, 1, 1500, 500, 100, 10)\n",
    "model.cuda()\n",
    "\n",
    "model_random = BinaryClassifier(N_DIM, 1, 1500, 500, 100, 10)\n",
    "model_random.cuda()\n",
    "\n",
    "RUN_COUNTER = 0 # If you re-instantiate the models, must restart the run counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, X_train, y_train, X_val, y_val, batch_size, n_epochs):\n",
    "    train_loss_list = []\n",
    "    validation_acc_list = []\n",
    "    n_batch = X_train.shape[0]//batch_size\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        # shuffle samples\n",
    "        shuffler = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[shuffler]\n",
    "        y_train = y_train[shuffler]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss_i = []\n",
    "        val_acc_i = []\n",
    "\n",
    "        for j in range(n_batch):\n",
    "            X_i = X_train[j*batch_size:(j+1)*batch_size]\n",
    "            y_i = y_train[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = torch.squeeze(model(X_i))\n",
    "            loss = loss_func(output, y_i)\n",
    "            train_loss_i.append(loss.item())\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Validation accuracy\n",
    "                output_val = torch.squeeze(model(X_val))\n",
    "                output_val = torch.round(output_val)\n",
    "                val_acc_i.append((output_val == y_val).sum().item()/y_val.shape[0])\n",
    "\n",
    "        train_loss_list.extend(train_loss_i)\n",
    "        validation_acc_list.extend(val_acc_i)\n",
    "    \n",
    "    duration = time.time() - tic\n",
    "    print(f\"Training: {duration//60} minutes, {duration%60} seconds\")\n",
    "    return train_loss_list, validation_acc_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss_list = []\n",
    "val_acc_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_run = LR\n",
    "n_ep_run = N_EPOCHS\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_run)\n",
    "loss_func = nn.BCELoss()\n",
    "train_loss_list_run, val_acc_list_run = train(model, optimizer, loss_func, X_train, y_train, X_val, y_val, BATCH_SIZE, n_ep_run)\n",
    "train_loss_list.extend(train_loss_list_run)\n",
    "val_acc_list.extend(val_acc_list_run)\n",
    "\n",
    "RUN_COUNTER += 1\n",
    "RUNS_DICT[RUN_COUNTER] = {\"N_EPOCHS\": n_ep_run, \"Learning rate\": lr_run}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Plot loss, validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_actual = 0\n",
    "for run, run_dict in RUNS_DICT.items():\n",
    "    n_epochs_actual += run_dict[\"N_EPOCHS\"]\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(range(len(train_loss_list)), train_loss_list, alpha=0.3)\n",
    "plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss vs. Epochs\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(val_acc_list)), val_acc_list)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Validation accuracy vs. Epochs\")\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate performance on test-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_test = torch.squeeze(model(X_test))\n",
    "    output_test = torch.round(output_test)\n",
    "    \n",
    "def get_metrics(y_pred, y_true):\n",
    "    true_pos = sum((y_pred==y_true)&(y_pred==1))\n",
    "    true_neg = sum((y_pred==y_true)&(y_pred==0))\n",
    "    false_pos = sum((y_pred!=y_true)&(y_pred==1))\n",
    "    false_neg = sum((y_pred!=y_true)&(y_pred==0))\n",
    "\n",
    "    acc = (true_pos+true_neg)/(true_pos+true_neg+false_pos+false_neg)\n",
    "    prec = true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos+false_neg)\n",
    "    f1 = 2*prec*recall/(prec+recall)\n",
    "    return acc, prec, recall, f1\n",
    "\n",
    "acc, prec, recall, f1 = get_metrics(output_test, y_test)\n",
    "p_pos_pred = sum(output_test==1)/len(output_test)\n",
    "p_pos_true = sum(y_test==1)/len(y_test)\n",
    "\n",
    "print(f\"Test accuracy: {100*acc:.2f}%\")\n",
    "print(f\"Test precision: {100*prec:.2f}%\")\n",
    "print(f\"Test recall: {100*recall:.2f}%\")\n",
    "print(f\"Test F1 score: {f1:.2f}\")\n",
    "print(f\"\\n\\nProportion of predictions that are positive: {100*p_pos_pred:.2f}%\")\n",
    "print(f\"Proportion of truth-labels that are positive: {100*p_pos_true:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Compute monte-carlo p-values of model's performance (controlling for the same proportion of predicted positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS = True\n",
    "VARY_P_POS = None\n",
    "\n",
    "def monte_carlo_bin_pred(y_truth, p_pos_pred, n_mc_samples, vary_p_pos): # generates random binary monte-carlo predictions, with the given probability of a positive prediction\n",
    "    mc_predictions = np.zeros(shape=(n_mc_samples, len(y_truth)))\n",
    "    for i in range(n_mc_samples):\n",
    "        if vary_p_pos:\n",
    "            p_pos_pred_i = np.random.choice(np.linspace(max(0, p_pos_pred-vary_p_pos), min(p_pos_pred+vary_p_pos,1), 100), size=1)[0]\n",
    "            pred_i = np.random.choice([0, 1], size=len(y_truth), p=[1-p_pos_pred_i, p_pos_pred_i])\n",
    "        else:\n",
    "            pred_i = np.random.choice([0, 1], size=len(y_truth), p=[1-p_pos_pred, p_pos_pred])\n",
    "        mc_predictions[i] = pred_i\n",
    "    return mc_predictions\n",
    "\n",
    "\n",
    "def monte_carlo_performance(y_truth, p_pos_pred, n_mc_samples, plots, vary_p_pos): # Computes monte-carlo performance metrics for a given probability of a positive prediction\n",
    "    mc_predictions = monte_carlo_bin_pred(y_truth, p_pos_pred, n_mc_samples, vary_p_pos)\n",
    "\n",
    "    acc = np.zeros(shape=(n_mc_samples))\n",
    "    prec = np.zeros(shape=(n_mc_samples))\n",
    "    recall = np.zeros(shape=(n_mc_samples))\n",
    "    f1 = np.zeros(shape=(n_mc_samples))\n",
    "    for i in range(n_mc_samples):\n",
    "        acc[i], prec[i], recall[i], f1[i] = get_metrics(mc_predictions[i], y_truth) \n",
    "\n",
    "    if plots:\n",
    "        plt.figure()\n",
    "        plt.hist(acc, bins=40)\n",
    "        plt.title(\"Monte-Carlo Accuracy\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(prec, bins=40)\n",
    "        plt.title(\"Monte-Carlo Precision\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(recall, bins=40)\n",
    "        plt.title(\"Monte-Carlo Recall\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(f1, bins=40)\n",
    "        plt.title(\"Monte-Carlo F1-Scores\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    return acc, prec, recall, f1\n",
    "    \n",
    "\n",
    "def get_performance(y_pred, y_truth, p_pos_pred, n_mc_samples=1000, plots=False, vary_p_pos=None): # Computes performance metric of given predictions, and the monte-carlo p-values of those metrics\n",
    "    acc, prec, recall, f1 = get_metrics(y_pred, y_truth)\n",
    "    mc_acc, mc_prec, mc_recall, mc_f1 = monte_carlo_performance(y_truth, p_pos_pred, n_mc_samples, plots, vary_p_pos)\n",
    "\n",
    "    p_acc = (np.sum(acc < mc_acc)+1)/(n_mc_samples+1)\n",
    "    p_prec = (np.sum(prec < mc_prec)+1)/(n_mc_samples+1)\n",
    "    p_recall = (np.sum(recall < mc_recall)+1)/(n_mc_samples+1)\n",
    "    p_f1 = (np.sum(f1 < mc_f1)+1)/(n_mc_samples+1)\n",
    "\n",
    "    performance_dict = {\"Accuracy\": (acc, p_acc),\n",
    "                        \"Precision\": (prec, p_prec),\n",
    "                        \"Recall\": (recall, p_recall),\n",
    "                        \"F1\": (f1, p_f1)}\n",
    "    return performance_dict\n",
    "\n",
    "tic = time.time()\n",
    "performance_dict = get_performance(output_test.cpu().numpy(), y_test.cpu().numpy(), p_pos_pred.item(), n_mc_samples=10000, plots=PLOTS, vary_p_pos=VARY_P_POS)\n",
    "duration = time.time() - tic\n",
    "print(f\"Duration: {duration//60} minutes, {duration%60} seconds\")   \n",
    "\n",
    "for key, value in performance_dict.items():\n",
    "    print(f\"{key}: ({100*value[0]:.2f}%, p={value[1]:.5f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Compare to randomly-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "TEST_RANDOM_ACTUAL = False # Allows user to change their mind and not run random model\n",
    "if TEST_RANDOM:     \n",
    "\n",
    "    train_loss_list_random = []\n",
    "    val_acc_list_random = []\n",
    "    # Train random model for same epochs, learning rate as true model\n",
    "    for run, run_dict in RUNS_DICT.items():\n",
    "\n",
    "        # Create random labels \n",
    "        shuffler = np.random.permutation(y_train.shape[0])\n",
    "        y_train_random = y_train[shuffler]\n",
    "\n",
    "        optimizer_random = torch.optim.Adam(model_random.parameters(), lr=run_dict[\"Learning rate\"])\n",
    "        loss_func = nn.BCELoss()\n",
    "        train_loss_list_random_run, val_acc_list_random_run = train(model_random, optimizer_random, loss_func, X_train, y_train_random, X_val, y_val, BATCH_SIZE, run_dict[\"N_EPOCHS\"])\n",
    "        train_loss_list_random.extend(train_loss_list_random_run)\n",
    "        val_acc_list_random.extend(val_acc_list_random_run)\n",
    "\n",
    "    ## Plot loss, validation accuracy\n",
    "    plt.figure()\n",
    "    plt.scatter(range(len(train_loss_list)), train_loss_list, alpha=0.3)\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Random Model: Training Loss vs. Epochs\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(val_acc_list_random)), val_acc_list_random)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Random Model: Validation Accuracy vs. Epochs\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    ## Evaluate on test-set\n",
    "    with torch.no_grad():\n",
    "        output_test_random = torch.squeeze(model_random(X_test))\n",
    "        output_test_random = torch.round(output_test_random)\n",
    "    acc_random, prec_random, recall_random, f1_random = get_metrics(output_test_random, y_test)\n",
    "    p_pos_pred_random = sum(output_test_random==1)/len(output_test_random)\n",
    "\n",
    "    print(f\"Test accuracy random: {100*acc_random:.2f}%\")\n",
    "    print(f\"Test precision random: {100*prec_random:.2f}%\")\n",
    "    print(f\"Test recall random: {100*recall_random:.2f}%\")\n",
    "    print(f\"Test F1 score random: {f1_random:.2f}\")\n",
    "    print(f\"\\n\\nProportion of predictions that are positive: {100*p_pos_pred_random:.2f}%\")\n",
    "    print(f\"Proportion of truth-labels that are positive: {100*p_pos_true:.2f}%\")\n",
    "\n",
    "    ## Compute monte carlo values\n",
    "    tic = time.time()\n",
    "    performance_dict_random = get_performance(output_test_random.cpu().numpy(), y_test.cpu().numpy(), p_pos_pred.item(), n_mc_samples=10000, plots=PLOTS, vary_p_pos=VARY_P_POS)\n",
    "    duration = time.time() - tic\n",
    "    print(f\"Monte Carlo: {duration//60} minutes, {duration%60} seconds\")\n",
    "\n",
    "    for key, value in performance_dict_random.items():\n",
    "        print(f\"{key}: ({100*value[0]:.2f}%, p={value[1]:.5f})\")\n",
    "\n",
    "    TEST_RANDOM_ACTUAL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save model and about.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_runs = [i for i in os.listdir(MLP_DIR) if \".pt\" in i]\n",
    "if len(prev_runs) == 0:\n",
    "    max_run = 0\n",
    "else:\n",
    "    max_run = int(prev_runs[-1].replace(\".pt\", \"\")[-1])\n",
    "RUN_PATH = os.path.join(MLP_DIR, f\"run_{max_run+1}.pt\")\n",
    "torch.save(model.state_dict(), RUN_PATH)\n",
    "\n",
    "RUN_ABOUT_PATH = RUN_PATH.replace(\".pt\", \".txt\")\n",
    "with open(RUN_ABOUT_PATH, \"w\") as f:\n",
    "    # Random seeds for reproducibility\n",
    "    f.write(f\"Numpy random seed: {NP_RANDOM_SEED}\")\n",
    "    f.write(f\"\\nTorch random seed: {TORCH_RANDOM_SEED}\")\n",
    "    \n",
    "    # Physical constants\n",
    "    f.write(f\"\\n\\nN: {N}\\nJ: {J}\\nFIRST_NONZERO: {FIRST_NONZERO}\") \n",
    "\n",
    "    # Training parameters\n",
    "    f.write(f\"\\n\\nModel: {model._modules}\\nN_SAMPLES: {N_SAMPLES}\\nR_TRAIN: {R_TRAIN}\\nBATCH_SIZE: {BATCH_SIZE}\") # model parameters\n",
    "    f.write(\"\\nRUNS_DICT\")\n",
    "    for k, v in RUNS_DICT.items():\n",
    "        f.write(f\"\\n  {k}: {v}\")\n",
    "\n",
    "    # Model performance\n",
    "    f.write(f\"\\n\\nPerformance: P_POS_TRUE={p_pos_true}\\n  Accuracy: {acc}\\n  Precision: {prec}\\n  Recall: {recall}\\n  F1: {f1}\\n P_POS_PRED: {p_pos_pred}\") # model performance\n",
    "    f.write(\"\\n\\nPerformance p-values at \" + f\"VARY_P_POS = {VARY_P_POS}\")\n",
    "    for k, v in performance_dict.items():\n",
    "        f.write(f\"\\n  {k}: ({100*v[0]:.2f}%, p={v[1]})\")\n",
    "\n",
    "    # Random model performance\n",
    "    if TEST_RANDOM and TEST_RANDOM_ACTUAL:\n",
    "        f.write(f\"\\n\\nRANDOM MODEL Performance: P_POS_T\\n  Accuracy: {acc_random}\\n  Precision: {prec_random}\\n  Recall: {recall_random}\\n  F1: {f1_random}\\n P_POS_PRED: {p_pos_pred_random}\") # random model performance\n",
    "        f.write(\"\\n\\nRANDOM MODEL Performance p-values at \" + f\"VARY_P_POS = {VARY_P_POS}\")\n",
    "        for k, v in performance_dict_random.items():\n",
    "            f.write(f\"\\n  {k}: ({100*v[0]:.2f}%, p={v[1]})\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys417",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

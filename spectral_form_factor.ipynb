{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import sparse\n",
    "from scipy import linalg\n",
    "from scipy.linalg import expm, sinm, cosm\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit, njit, prange\n",
    "\n",
    "#################### Macros #############################\n",
    "np.random.seed(0)\n",
    "\n",
    "# Physical constants\n",
    "K=10 # number of fermionic modes <-- They use K=17 in the paper\n",
    "BETA = 5 # inverse temperature\n",
    "N = 2*K # number of fermions\n",
    "N_DIM = 2**K # Hilbert space dimensions\n",
    "J=4 # ~\"energy scale\" <-- Their J not given in the paper, nor whether we're even in strong or weak-coupling regime. Maybe all that matters is t*J, which is what they plot?\n",
    "Q=4 # order of coupling <-- Their q also not given. 4 is pretty generic, so let's stick with that \n",
    "E_ORDER = 19 # Order to keep of matrix-exponential expansion. Also not given in the paper. \n",
    "\n",
    "# Directory to save sample Hamiltonians\n",
    "H_DIR = os.path.join(\"Simulated Hamiltonians\", f\"H4_K{K}_J{J}_Q{Q}\")\n",
    "os.makedirs(H_DIR, exist_ok=True)\n",
    "\n",
    "N_SAMPLES = 160 # number of samples to generate\n",
    "N_JOBS = 20 # number of jobs to run in parallel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pre-computed sample Hamiltonians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See notebook: \"computing_eigenvalues_for_unfolding.ipynb\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "H_all= np.zeros((N_SAMPLES, N_DIM, N_DIM), dtype=np.complex128) \n",
    "#iv_all = np.zeros((N_SAMPLES, N_DIM), dtype=np.float64) # iv = eigenvalues\n",
    "for i in range(N_SAMPLES):\n",
    "    H_all[i] = np.load(os.path.join(H_DIR, f\"H4_{i+1}.npy\"))\n",
    "    #iv_all[i] = np.linalg.eigvalsh(H_all[i])\n",
    "\n",
    "iv_all = np.load(os.path.join(H_DIR, \"eigenvalues_all.npy\"))\n",
    "#np.save(os.path.join(H_DIR, \"eigenvalues_all.npy\"), iv_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_index = np.random.choice(N_SAMPLES)\n",
    "H_test = H_all[test_index]\n",
    "iv_test = iv_all[test_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dense: 0.0 minutes, 0.813676118850708 seconds\n"
     ]
    }
   ],
   "source": [
    "def matrix_exponential(A, n): # A = matrix, n = order of expansion\n",
    "    out = np.zeros(A.shape, dtype=np.complex128)\n",
    "    last_term = np.identity(A.shape[0])\n",
    "    out += last_term\n",
    "    for i in range(1, n+1):\n",
    "        last_term = last_term @ A / i\n",
    "        out += last_term\n",
    "        \n",
    "    return out\n",
    "\n",
    "tic = time.time()\n",
    "e_to_H = matrix_exponential(H_test, E_ORDER)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Dense: {duration//60} minutes, {duration%60} seconds\")\n",
    "\n",
    "later = \"\"\"\n",
    "def matrix_exponential_sparse(A, n): # A = matrix, n = order of expansion\n",
    "    out = sparse.csr_array(np.zeros(A.shape, dtype=np.complex128))\n",
    "    last_term = sparse.csr_array(np.identity(A.shape[0]))\n",
    "    out += last_term\n",
    "    for i in range(1, n+1):\n",
    "        last_term = last_term @ A / i\n",
    "        out += last_term\n",
    "        \n",
    "    return out\n",
    "\n",
    "H_test_sparse = sparse.csr_array(H_test)\n",
    "tic = time.time()\n",
    "e_to_H_sparse = matrix_exponential_sparse(H_test_sparse, E_ORDER)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Sparse: {duration//60} minutes, {duration%60} seconds\")\n",
    "\n",
    "\n",
    "print(np.allclose(e_to_H, e_to_H_sparse.toarray()))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare how close self-defined matrix-exponential is with scipy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scipy: 0.0 minutes, 2.4136734008789062 seconds\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "e_to_H_scipy = expm(H_test)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Scipy: {duration//60} minutes, {duration%60} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scipy function too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E_ORDER: 19\n",
      "allclose:  True\n",
      "max_abs_diff:  1.0674152228773437e-08\n",
      "sum_abs_diff:  0.00039559482875329655\n"
     ]
    }
   ],
   "source": [
    "print(f\"E_ORDER: {E_ORDER}\")\n",
    "print(\"allclose: \", np.allclose(e_to_H, e_to_H_scipy))\n",
    "\n",
    "abs_diff = np.abs(e_to_H - e_to_H_scipy)\n",
    "print(\"max_abs_diff: \", np.max(np.max(abs_diff, axis=0), axis=0))\n",
    "print(\"sum_abs_diff: \", np.sum(np.sum(abs_diff, axis=0), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the sum of the absolute difference to be at least (a few orders of magnitude) smaller than the Hilbert space dimension (4096 for K=12, 1024 for K=10)\n",
    "\n",
    "For K=12: E_ORDER=13 seems optimal\n",
    "- E_ORDER=6: sum_abs_diff = 248906, time = 15.4 seconds\n",
    "- E_ORDER=10: sum_abs_diff = 8483, time = 23.5 seconds\n",
    "- E_ORDER=12: sum_abs_diff = 970, time = 30 seconds\n",
    "- E_ORDER=13: sum_abs_diff = 6e-4, time = 31 seconds\n",
    "- E_ORDER=15: sum_abs_diff = 5e-5, time = 36.6 seconds\n",
    "- E_ORDER=20: sum_abs_diff = 1.5e-8, time = 58 seconds\n",
    "\n",
    "For K=10: E_ORDER=19 seems optimal\n",
    "- E_ORDER=19: sum_abs_diff = 4e-4, time=0.81 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick check to see if sparse matrix-exponential is faster (it's probably not)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "later = \"\"\"\n",
    "print(f\"E_ORDER: {E_ORDER}\")\n",
    "\n",
    "tic = time.time()\n",
    "e_to_H_sparse = matrix_exponential_sparse(H_test_sparse, E_ORDER)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Dense: {duration//60} minutes, {duration%60} seconds\")\n",
    "\n",
    "e_to_H_sparse = e_to_H_sparse.toarray()\n",
    "print(\"allclose: \", np.allclose(e_to_H_sparse, e_to_H_scipy))\n",
    "\n",
    "abs_diff = np.abs(e_to_H_sparse - e_to_H_scipy)\n",
    "print(\"max_abs_diff: \", np.max(np.max(abs_diff, axis=0), axis=0))\n",
    "print(\"sum_abs_diff: \", np.sum(np.sum(abs_diff, axis=0), axis=0))\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Not sure *why* it's slower... \n",
    "Also has larger error relative to non-sparse version..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define spectral form factor, disorder-averaged analogs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Straightforward version, proof of concept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single timestep, each disorder-term requires 2 (sequential?)  matrix exponentials (and 2 traces, a conjugate, etc.). Expect that each timestep should therefore take 2*N_SAMPLES*E_ORDER_TIME (or 1*...), where E_ORDER_TIME is the time it takes to compute a matrix exponential for the given parameters K, E_ORDER, etc...\n",
    "\n",
    "N_SAMPLES=160 is standard so far\n",
    "- For K=10, E_ORDER=19: exp_time = 1.61*160 = 257 seconds ~ 4 minutes\n",
    "- For K=12, E_ORDER=13: exp_time = 31*160 = 4960 seconds ~ 82 minutes!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single time-step, no parallelization: 11.0 minutes, 29.364864826202393 seconds\n"
     ]
    }
   ],
   "source": [
    "def Z(H): # <-- RETURNS SCALAR\n",
    "    return np.trace(matrix_exponential(-BETA*H, E_ORDER))\n",
    "\n",
    "def Zt(H,t): # <-- RETURNS SCALAR\n",
    "    return np.trace(matrix_exponential(-(BETA+1j*t)*H, E_ORDER))\n",
    "\n",
    "def g(H_all, t): # <-- RETURNS SCALAR\n",
    "    g_num_all = np.zeros(H_all.shape[0], dtype=np.complex128)\n",
    "    sqrt_g_den_all = np.zeros(H_all.shape[0], dtype=np.complex128)\n",
    "    for i in range(H_all.shape[0]):\n",
    "        H_i = H_all[i]\n",
    "        Zt_i = Zt(H_i, t)\n",
    "        g_num_all[i] = Zt_i*np.conj(Zt_i)\n",
    "\n",
    "        sqrt_g_den_all[i] = Z(H_i)\n",
    "    \n",
    "    g_num = np.mean(g_num_all, axis=0)\n",
    "    g_den = np.mean(sqrt_g_den_all, axis=0)**2\n",
    "    return g_num/g_den\n",
    "\n",
    "# test for a single timestep\n",
    "test_t = 12\n",
    "tic = time.time()\n",
    "g_test = g(H_all, test_t)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Single time-step, no parallelization: {duration//60} minutes, {duration%60} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13.871971220532524-7.366260392482353e-21j)\n",
      "3.871201010907891\n"
     ]
    }
   ],
   "source": [
    "print(np.log10(g_test))\n",
    "print(np.log(test_t*J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Per figure 1 in the paper (and per common sense), $g<10^0=1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Taking too long so I quit it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallelized version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Should take as long as sequential version divided by N_JOBS\n",
    "\n",
    "N_SAMPLES=160, N_JOBS=20 is standard so far\n",
    "- For K=10, E_ORDER=19: exp_time = 1.61*160/20 = 12.85 seconds \n",
    "- For K=12, E_ORDER=13: exp_time = 31*160 = 248 seconds ~ 4 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Z(H):\n",
    "    return np.trace(matrix_exponential(-BETA*H, E_ORDER))\n",
    "\n",
    "def Zt(H,t):\n",
    "    return np.trace(matrix_exponential(-(BETA+1j*t)*H, E_ORDER))\n",
    "\n",
    "def g_i(H_all, t, g_num_all, sqrt_g_den_all, i): # Computes Zt(H,t)*Z*(H,t) for a single sample, sends result to g_num_all[i], computes Z(H) for a single sample, sends result to sqrt_g_den_all[i]\n",
    "    H_i = H_all[i]\n",
    "    Zt_i = Zt(H_i, t)\n",
    "    g_num_all[i] = Zt_i*np.conj(Zt_i)\n",
    "    sqrt_g_den_all[i] = Z(H_i)\n",
    "\n",
    "def g_par(H_all, t, n_jobs=N_JOBS): # <-- makes more sense to parallelize over samples, not time-steps\n",
    "    g_num_all = np.zeros(H_all.shape[0], dtype=np.complex128)\n",
    "    sqrt_g_den_all = np.zeros(H_all.shape[0], dtype=np.complex128)\n",
    "    Parallel(n_jobs=n_jobs)(delayed(g_i)(H_all, t, g_num_all, sqrt_g_den_all, i) for i in range(H_all.shape[0]))\n",
    "\n",
    "    g_num = np.mean(g_num_all, axis=0)\n",
    "    g_den = np.mean(sqrt_g_den_all, axis=0)**2\n",
    "    return g_num/g_den\n",
    "\n",
    "# test for a single timestep\n",
    "tic = time.time()\n",
    "g_par_test = g_par(H_all, test_t)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Single time-step, with parallelization: {duration//60} minutes, {duration%60} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.log10(g_test))\n",
    "print(np.log(test_t*J))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that sequential and parallel version give the same results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g_test, g_par_test)\n",
    "abs_diff = np.abs(g_test - g_par_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For all timesteps..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_range_test = np.linspace(0,1,10)\n",
    "g_range_test = np.zeros(t_range_test.shape, dtype=np.complex128)\n",
    "\n",
    "tic = time.time()\n",
    "for i in len(g_range_test):\n",
    "    t_i = t_range_test[i]\n",
    "    g_i = g(H_all, t_i)\n",
    "    g_range_test[i] = g_i\n",
    "toc = time.time()\n",
    "duration = time.time() - tic "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys417",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

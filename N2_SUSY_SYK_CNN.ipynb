{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file-handling\n",
    "import os \n",
    "\n",
    "# user status updates\n",
    "import time\n",
    "from time import gmtime, strftime\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, Latex\n",
    "from datetime import datetime\n",
    "\n",
    "# the holy trinity of python data science\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# scipy\n",
    "import scipy\n",
    "from scipy import sparse, linalg, fft\n",
    "from scipy.linalg import expm, sinm, cosm\n",
    "import scipy.integrate as integrate\n",
    "from scipy.integrate import quad\n",
    "\n",
    "# torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# parallelization, memory management\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit, njit, prange\n",
    "import copy\n",
    "\n",
    "# itertools\n",
    "import itertools\n",
    "\n",
    "############ Macros ###############\n",
    "NP_RANDOM_SEED = 0\n",
    "TORCH_RANDOM_SEED = 0\n",
    "np.random.seed(NP_RANDOM_SEED)\n",
    "torch.manual_seed(TORCH_RANDOM_SEED)\n",
    "\n",
    "# Physical constants\n",
    "N = 8 # number of fermions\n",
    "J = 100 # ~\"energy scale\" of couplings\n",
    "Q_COUPLING = 3 # order of coupling, don't want to use the letter 'Q' because that denotes the supercharge\n",
    "N_DIM = 2**N # Hilbert space dimension \n",
    "FIRST_NONZERO = 0.15 # first nonzero eigenvalue, for our intents and purposes\n",
    "\n",
    "# Computer stuff\n",
    "N_SAMPLES = 300 # number of samples to generate\n",
    "N_JOBS = 20 # number of jobs to run in parallel\n",
    "MC_SIMULATED = False\n",
    "MC_RANDOM_SIMULATED = False\n",
    "\n",
    "# Model stuff\n",
    "LR = 1e-5 # learning rate\n",
    "BATCH_SIZE = 50 # batch size for training the neural network\n",
    "N_EPOCHS = 5 # number of epochs to train the neural network\n",
    "R_TRAIN = 0.5 # desired ratio of zero eigenvalues to total eigenvalues IN TRAINING SET\n",
    "TEST_RANDOM = True # whether to train and test the randomly-trained model\n",
    "\n",
    "# Directories\n",
    "N2_SUSY_DIR = os.path.join(\"Excel\", \"N2_SUSY_SYK\")\n",
    "RESULT_DIR = os.path.join(N2_SUSY_DIR, \"Simulated Hamiltonians\", f\"N{N}_J{J}\")\n",
    "CNN_DIR = os.path.join(N2_SUSY_DIR, \"CNN\", f\"N{N}_J{J}\")\n",
    "os.makedirs(CNN_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##       1.1 Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IV = N_SAMPLES*N_DIM\n",
    "ivals_all = np.zeros(shape=(N_IV,), dtype=np.float64)\n",
    "ivecs_all = np.zeros(shape=(N_IV, N_DIM), dtype=np.complex128)\n",
    "\n",
    "for i in range(N_SAMPLES):\n",
    "    ivals_i = np.load(os.path.join(RESULT_DIR, f\"ivals_{i}.npy\"))\n",
    "    ivecs_i = np.load(os.path.join(RESULT_DIR, f\"ivecs_{i}.npy\"))\n",
    "    ivals_all[i*N_DIM:(i+1)*N_DIM] = ivals_i\n",
    "    ivecs_all[i*N_DIM:(i+1)*N_DIM,:] = ivecs_i\n",
    "\n",
    "labels_all = (ivals_all>=FIRST_NONZERO).astype(int) # zero-energy eigenvectors labeled with 0, finite-energy eigenvectors labeled with 1\n",
    "print(f\"ivals_all.shape: {ivals_all.shape}\")\n",
    "print(f\"ivecs_all.shape: {ivecs_all.shape}\")\n",
    "print(f\"labels_all.shape: {labels_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Convert (1xN_DIM) complex into (1xN_DIMx2) real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch doesn't currently have handling for complex numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ivecs_all_new = np.zeros(shape=(N_IV, 2, N_DIM), dtype=np.float64)\n",
    "\n",
    "for i in range(N_IV):\n",
    "    ivecs_all_i = ivecs_all[i]\n",
    "    ivecs_all_new[i,0,:] = ivecs_all_i.real\n",
    "    ivecs_all_new[i,1,:] = ivecs_all_i.real\n",
    "\n",
    "ivecs_all = ivecs_all_new\n",
    "del ivecs_all_new\n",
    "\n",
    "print(f\"ivecs_all.shape = {ivecs_all.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Train-test-validate split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first, random shuffle\n",
    "shuffler = np.random.permutation(N_IV)\n",
    "ivecs_all = ivecs_all[shuffler]\n",
    "labels_all = labels_all[shuffler]\n",
    "\n",
    "p_train = 0.7\n",
    "n_train = int(p_train*N_IV)\n",
    "X_train = ivecs_all[:n_train]\n",
    "y_train = labels_all[:n_train]\n",
    "\n",
    "p_val = 0.2\n",
    "n_val = int(p_val*N_IV)\n",
    "X_val = ivecs_all[n_train:n_train+n_val]\n",
    "y_val = labels_all[n_train:n_train+n_val]\n",
    "\n",
    "p_test = 0.1\n",
    "n_test = int(p_test*N_IV)\n",
    "X_test = ivecs_all[n_train+n_val:]\n",
    "y_test = labels_all[n_train+n_val:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Load extra zero-energy data to balance training dataset, if specified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_EXTRA_ZEROS = True\n",
    "if LOAD_EXTRA_ZEROS:\n",
    "\n",
    "    # Load the data \n",
    "    ivals_all_0 = []\n",
    "    ivecs_all_0 = []\n",
    "    for i in range(N_SAMPLES, N_SAMPLES+100):\n",
    "        ivals_i = np.load(os.path.join(RESULT_DIR, f\"ivals_{i}.npy\"))\n",
    "        ivecs_i = np.load(os.path.join(RESULT_DIR, f\"ivecs_{i}.npy\"))\n",
    "\n",
    "        ivals_i_0_idx = np.where(ivals_i < FIRST_NONZERO)[0]\n",
    "        #print(f\"ivals_i_0_idx = {ivals_i_0_idx}\")\n",
    "        ivals_i_0 = [ivals_i[i] for i in ivals_i_0_idx]\n",
    "        ivecs_i_0 = [ivecs_i[i,:] for i in ivals_i_0_idx]\n",
    "        ivals_all_0.extend(ivals_i_0)\n",
    "        ivecs_all_0.extend(ivecs_i_0)\n",
    "\n",
    "    ivals_all_0 = np.array(ivals_all_0)\n",
    "    ivecs_all_0 = np.array(ivecs_all_0)\n",
    "\n",
    "    # Reshape complex 1xN_DIM into real 2xN_DIM\n",
    "    # ivecs_all_0_new = np.zeros(shape=(ivecs_all_0.shape[0], N_DIM), dtype=np.float64)\n",
    "    # for i in range(ivecs_all_0.shape[0]):\n",
    "    #     ivecs_all_0_i = ivecs_all_0[i]\n",
    "    #     ivecs_all_0_new[i] = np.concatenate((ivecs_all_0_i.real, ivecs_all_0_i.imag), axis=0)\n",
    "    # ivecs_all_0 = ivecs_all_0_new\n",
    "    # del ivecs_all_0_new\n",
    "    # print(f\"ivals_all_0.shape = {ivals_all_0.shape}\")\n",
    "    # print(f\"ivecs_all_0.shape = {ivecs_all_0.shape}\")\n",
    "\n",
    "    ivecs_all_0_new = np.zeros(shape=(ivecs_all_0.shape[0], 2, N_DIM), dtype=np.float64)\n",
    "    for i in range(ivecs_all_0.shape[0]):\n",
    "        ivecs_all_0_i = ivecs_all_0[i]\n",
    "        ivecs_all_0_new[i,0,:] = ivecs_all_0_i.real\n",
    "        ivecs_all_0_new[i,1,:] = ivecs_all_0_i.real\n",
    "    ivecs_all_0 = ivecs_all_0_new\n",
    "    del ivecs_all_0_new\n",
    "    print(f\"ivecs_all_0.shape = {ivecs_all_0.shape}\")\n",
    "\n",
    "\n",
    "    X_train_0 = ivecs_all_0\n",
    "    y_train_0 = np.array(ivals_all_0 < FIRST_NONZERO, dtype=np.int64)\n",
    "    X_train = np.concatenate((X_train, X_train_0), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_train_0), axis=0)\n",
    "\n",
    "    print(f\"X_train.shape = {X_train.shape}\")\n",
    "    print(f\"y_train.shape = {y_train.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.5 Balance training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$r_{train} = $ desired proportion of zero-energy eigenvectors\n",
    "\n",
    "$N_0 =$ number of zero-energy eigenvectors\n",
    "\n",
    "$N_f =$ number of finite-energy eigenvectors\n",
    "\n",
    "$N_{add}$ number of zero-energy eigenvectors we must add to achieve $r_{want}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To find $N_{add}$, we start off with an identity:\n",
    "\n",
    "$r_{train} \\equiv \\frac{N_0+N_{add}}{N_0+N_f+N_{add}} $\n",
    "\n",
    "$N_0+N_{add} = r_{train} (N_0+N_f+N_{add})$\n",
    "\n",
    "$N_0+N_{add} = r_{train}*N_0+r_{train}*N_f+r_{train}*N_{add}$\n",
    "\n",
    "$N_{add}(1-r_{train}) = N_0(r_{train}-1)+r_{train}*N_f$\n",
    "\n",
    "\n",
    "And finally,\n",
    "\n",
    "$N_{add} = \\frac{N_0(r_{train}-1)+N_f*r_{train}}{1-r_{train}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_0 = sum(y_train==0)\n",
    "N_f = sum(y_train==1)\n",
    "N_add = (N_0*(R_TRAIN-1)+N_f*R_TRAIN)/(1-R_TRAIN)\n",
    "\n",
    "X_train_0 = X_train[y_train==0]\n",
    "X_train_0_clone_idx = np.random.choice(range(X_train_0.shape[0]), size=int(N_add), replace=True)\n",
    "X_train_0_clone = copy.deepcopy(X_train_0[X_train_0_clone_idx])\n",
    "y_train_0_clone = np.zeros(shape=(X_train_0_clone.shape[0],), dtype=np.int32)\n",
    "\n",
    "X_train = np.concatenate((X_train, X_train_0_clone), axis=0)\n",
    "y_train = np.concatenate((y_train, y_train_0_clone), axis=0)\n",
    "\n",
    "print(f\"X_train.shape: {X_train.shape}\")\n",
    "print(f\"N_0: {sum(y_train==0)/len(y_train)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.6 Convert to Torch tensors that CNN can read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float().cuda()\n",
    "y_train = torch.from_numpy(y_train).float().cuda()\n",
    "\n",
    "X_val = torch.from_numpy(X_val).float().cuda()\n",
    "y_val = torch.from_numpy(y_val).float().cuda()\n",
    "\n",
    "X_test = torch.from_numpy(X_test).float().cuda()\n",
    "y_test = torch.from_numpy(y_test).float().cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_i = X_train[0:3]\n",
    "print(f\"X_i.shape: {X_i.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv1d(2, 1, N_DIM//16, stride=1)\n",
    "conv1.cuda()\n",
    "X1 = conv1(X_i)\n",
    "print(f\"X1.shape: {X1.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp1 = nn.MaxPool1d(kernel_size=N_DIM//64, stride=1)\n",
    "mp1.cuda()\n",
    "X2 = mp1(X1)\n",
    "print(f\"X2.shape = {X2.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop1 = nn.Dropout(p=0.5)\n",
    "drop1.cuda()\n",
    "X3 = drop1(X2)\n",
    "print(f\"X3.shape = {X3.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryClassifierCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \n",
    "        super(BinaryClassifierCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(2, 1, N_DIM//16, stride=1)\n",
    "        self.mp1 = nn.MaxPool1d(kernel_size=N_DIM//64, stride=1)\n",
    "        self.drop1 = nn.Dropout(p=0.5)\n",
    "        #self.conv2 = nn.Conv1d(2, 2, N_DIM//8, stride=1)\n",
    "        #self.conv3 = nn.Conv1d(2, 1, N_DIM//16, stride=2)\n",
    "        #self.fc1 = nn.Linear(290, 100)\n",
    "        #self.fc2 = nn.Linear(100, 10)\n",
    "        #self.fc3 = nn.Linear(10, 1)\n",
    "        self.fc1 = nn.Linear(238, 100)\n",
    "        self.fc2 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        #self.fc4 = nn.Linear(10, 1)\n",
    "\n",
    "\n",
    "    # Function to pass data forward through network.\n",
    "    def forward(self, x):\n",
    "        out1 = F.relu(self.conv1(x))\n",
    "        out2 = self.drop1(self.mp1(out1))\n",
    "        # out2 = F.relu(self.conv2(out1))\n",
    "        # out3 = F.relu(self.conv3(out2))\n",
    "        # out4 = F.relu(self.fc1(out3))\n",
    "        # out5 = F.relu(self.fc2(out4))\n",
    "        # out6 = F.sigmoid(self.fc3(out5))\n",
    "        \n",
    "        # return torch.squeeze(out6, axis=1)\n",
    "\n",
    "        out3 = F.relu(self.fc1(out2))\n",
    "        out4 = F.relu(self.fc2(out3))\n",
    "        out5 = F.sigmoid(self.fc3(out4))\n",
    "        #out6 = F.sigmoid(self.fc4(out5))\n",
    "        return torch.squeeze(out5, axis=1)\n",
    "\n",
    "# Initialize model and random model \n",
    "model = BinaryClassifierCNN()\n",
    "model.cuda()\n",
    "\n",
    "model_random = BinaryClassifierCNN()\n",
    "model_random.cuda()\n",
    "\n",
    "\n",
    "# If you re-instantiate the models, must restart the run counters, run dicts, and loss lists.\n",
    "RUN_COUNTER = 0 \n",
    "RUNS_DICT = {}\n",
    "RUNS_DICT_RANDOM = {}\n",
    "\n",
    "mean_train_loss_list = []\n",
    "train_loss_list = []\n",
    "val_acc_list = []\n",
    "pos_pred_list = []\n",
    "\n",
    "mean_train_loss_list_random = []\n",
    "train_loss_list_random = []\n",
    "val_acc_list_random = []\n",
    "pos_pred_list_random = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_out = model(X_i)\n",
    "print(f\"X_out.shape = {X_out.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Define training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_func, X_train, y_train, X_val, y_val, batch_size, n_epochs):\n",
    "    mean_train_loss_list = []\n",
    "    train_loss_list = []\n",
    "    validation_acc_list = []\n",
    "    pos_pred_list = []\n",
    "    n_batch = X_train.shape[0]//batch_size\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in tqdm(range(n_epochs)):\n",
    "        # shuffle samples\n",
    "        shuffler = np.random.permutation(X_train.shape[0])\n",
    "        X_train = X_train[shuffler]\n",
    "        y_train = y_train[shuffler]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        train_loss_i = []\n",
    "        val_acc_i = []\n",
    "        pos_pred_i = []\n",
    "\n",
    "        for j in range(n_batch):\n",
    "            X_i = X_train[j*batch_size:(j+1)*batch_size]\n",
    "            y_i = y_train[j*batch_size:(j+1)*batch_size]\n",
    "\n",
    "            # Forward pass\n",
    "            output = torch.squeeze(model(X_i))\n",
    "            loss_ij = loss_func(output, y_i)\n",
    "            train_loss_i.append(loss_ij.item())\n",
    "\n",
    "            loss_ij.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                # Validation accuracy\n",
    "                output_val = torch.round(torch.squeeze(model(X_val))).detach().cpu().numpy()\n",
    "\n",
    "                val_acc_ij = (output_val == y_val.detach().cpu().numpy()).sum()/y_val.shape[0]\n",
    "                val_acc_i.append(val_acc_ij)\n",
    "\n",
    "                pos_pred_ij = sum(output_val)/len(output_val)\n",
    "                pos_pred_i.append(pos_pred_ij)\n",
    "\n",
    "        mean_train_loss_list.append(np.mean(np.array(train_loss_i)))\n",
    "        train_loss_list.extend(train_loss_i)\n",
    "        validation_acc_list.extend(val_acc_i)\n",
    "        pos_pred_list.extend(pos_pred_i)\n",
    "    \n",
    "    duration = time.time() - tic\n",
    "    print(f\"Training: {duration//60} minutes, {duration%60} seconds\")\n",
    "    return mean_train_loss_list, train_loss_list, validation_acc_list, pos_pred_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_run = LR\n",
    "n_ep_run = N_EPOCHS\n",
    "bs_run = BATCH_SIZE\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr_run)\n",
    "loss_func = nn.BCELoss()\n",
    "mean_train_loss_list_run, train_loss_list_run, val_acc_list_run, pos_pred_list_run = train(model, optimizer, loss_func, X_train, y_train, X_val, y_val, bs_run, n_ep_run)\n",
    "mean_train_loss_list.extend(mean_train_loss_list_run)\n",
    "train_loss_list.extend(train_loss_list_run)\n",
    "val_acc_list.extend(val_acc_list_run)\n",
    "pos_pred_list.extend(pos_pred_list_run)\n",
    "\n",
    "RUN_COUNTER += 1\n",
    "RUNS_DICT[RUN_COUNTER] = {\"Number of epochs\": n_ep_run, \"Learning rate\": lr_run, \"Batch size\": bs_run}\n",
    "for k,v in RUNS_DICT[RUN_COUNTER].items():\n",
    "    print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Plot loss, validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs_actual = 0\n",
    "for run, run_dict in RUNS_DICT.items():\n",
    "    n_epochs_actual += run_dict[\"Number of epochs\"]\n",
    "\n",
    "mean_train_xrange = np.linspace(0, len(train_loss_list), len(mean_train_loss_list))\n",
    "plt.figure()\n",
    "plt.scatter(range(len(train_loss_list)), train_loss_list, alpha=0.15, label=\"Raw training loss\", c=\"b\")\n",
    "plt.plot(mean_train_xrange, mean_train_loss_list, label=\"Epoch-averaged training loss\", c=\"r\")\n",
    "plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training Loss vs. Epochs\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(val_acc_list)), val_acc_list)\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Validation accuracy vs. Epochs\")\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(range(len(pos_pred_list)), pos_pred_list, label=\"Model predictions\")\n",
    "plt.plot(range(len(pos_pred_list)), [1-R_TRAIN for i in pos_pred_list], label=\"Training set\")\n",
    "R_VAL = (sum(y_val)/len(y_val)).item()\n",
    "plt.plot(range(len(pos_pred_list)), [R_VAL for i in pos_pred_list], label=\"Validation/Test set\")\n",
    "plt.ylabel(\"Proportion of predictions that are positive\")\n",
    "plt.xticks(ticks=np.linspace(0, len(train_loss_list), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.title(\"Proportion of positive-predictions (on validation-set) vs. Epochs\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Evaluate performance on test-set, with Monte-Carlo p-values controlling for the same ratio of 0's to 1's"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the Monte-Carlo p-values, we're basically checking: what are the odds of getting this level of performance by simply guessing 0's or 1's at random at the same proportion as the trained neural network? \n",
    "\n",
    "So the null hypothesis is:\n",
    "- The neural network only learned to predict a certain ratio of 0's-to-1's in order to minimize the loss function. \n",
    "\n",
    "And the alternative hypothesis is:\n",
    "- The neural network learned an actual (latent) structure/relationship within/between the zero-energy and finite-energy eigenvectors. \n",
    "\n",
    "If the null hypothesis is true, we should expect that randomly choosing 0's or 1's - at the same ratio as the trained model - should give similar performance to the trained model. Looking at the Monte-Carlo distribution of performance metrics (i.e. accuracy, precision, recall, F1), the performance of the trained model should not be too far from average. \n",
    "\n",
    "\n",
    "If the alternative hypothesis is true, randomly choosing 0's or 1's at the same ratio as the trained model will not reproduce the trained model's performance. The neural network's performance metrics should not seem average within in the Monte-Carlo distribution of performance metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computes binary classification performance metrics\n",
    "def get_metrics(y_true, y_pred): \n",
    "    true_pos = sum((y_pred==y_true)&(y_pred==1))\n",
    "    true_neg = sum((y_pred==y_true)&(y_pred==0))\n",
    "    false_pos = sum((y_pred!=y_true)&(y_pred==1))\n",
    "    false_neg = sum((y_pred!=y_true)&(y_pred==0))\n",
    "\n",
    "    acc = (true_pos+true_neg)/(true_pos+true_neg+false_pos+false_neg)\n",
    "    prec = true_pos/(true_pos+false_pos)\n",
    "    recall = true_pos/(true_pos+false_neg)\n",
    "    f1 = 2*prec*recall/(prec+recall)\n",
    "    return acc, prec, recall, f1\n",
    "\n",
    "# Generates random binary monte-carlo predictions, with the given probability of a positive prediction\n",
    "def monte_carlo_bin_pred(sample_size, n_samples, p_pos_pred, vary_p_pos): \n",
    "    mc_predictions = np.zeros(shape=(n_samples, sample_size))\n",
    "    for i in range(n_samples):\n",
    "        if vary_p_pos:\n",
    "            p_pos_pred_i = np.random.choice(np.linspace(max(0, p_pos_pred-vary_p_pos), min(p_pos_pred+vary_p_pos,1), 100), size=1)[0]\n",
    "            pred_i = np.random.choice([0, 1], size=sample_size, p=[1-p_pos_pred_i, p_pos_pred_i])\n",
    "        else:\n",
    "            pred_i = np.random.choice([0, 1], size=sample_size, p=[1-p_pos_pred, p_pos_pred])\n",
    "        mc_predictions[i] = pred_i\n",
    "    return mc_predictions\n",
    "\n",
    "# Computes (distribution of) monte-carlo performance metrics\n",
    "def monte_carlo_performance(y_truth, mc_preds, plots): \n",
    "    n_mc_samples = mc_preds.shape[0]\n",
    "\n",
    "    acc = np.zeros(shape=(n_mc_samples))\n",
    "    prec = np.zeros(shape=(n_mc_samples))\n",
    "    recall = np.zeros(shape=(n_mc_samples))\n",
    "    f1 = np.zeros(shape=(n_mc_samples))\n",
    "    for i in range(n_mc_samples):\n",
    "        acc[i], prec[i], recall[i], f1[i] = get_metrics(mc_preds[i], y_truth) \n",
    "\n",
    "    if plots:\n",
    "        plt.figure()\n",
    "        plt.hist(acc, bins=40)\n",
    "        plt.title(\"Monte-Carlo Accuracy\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(prec, bins=40)\n",
    "        plt.title(\"Monte-Carlo Precision\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(recall, bins=40)\n",
    "        plt.title(\"Monte-Carlo Recall\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "        plt.figure()\n",
    "        plt.hist(f1, bins=40)\n",
    "        plt.title(\"Monte-Carlo F1-Scores\")\n",
    "        plt.show()\n",
    "        plt.close()\n",
    "\n",
    "    mc_performance_dict = {\"Accuracy\": acc,\n",
    "                \"Precision\": prec,\n",
    "                \"Recall\": recall,\n",
    "                \"F1\": f1}\n",
    "    return mc_performance_dict\n",
    "    \n",
    "# Computes performance metric of given predictions, and the monte-carlo p-values of those metrics\n",
    "def get_performance(y_truth, y_pred, mc_performance_dict, plots=False): \n",
    "\n",
    "    acc, prec, recall, f1 = get_metrics(y_truth, y_pred) # model performance metrics \n",
    "    \n",
    "    # monte carlo performance metrics\n",
    "    mc_acc = mc_performance_dict[\"Accuracy\"]\n",
    "    mc_prec = mc_performance_dict[\"Precision\"]\n",
    "    mc_recall = mc_performance_dict[\"Recall\"]\n",
    "    mc_f1 = mc_performance_dict[\"F1\"]\n",
    "\n",
    "    n_mc_samples = len(mc_acc) # number of monte carlo samples\n",
    "    p_acc = (np.sum(acc < mc_acc)+1)/(n_mc_samples+1) # Accyracy p-value\n",
    "    p_prec = (np.sum(prec < mc_prec)+1)/(n_mc_samples+1) # Precision p-value\n",
    "    p_recall = (np.sum(recall < mc_recall)+1)/(n_mc_samples+1) # Recall p-value\n",
    "    p_f1 = (np.sum(f1 < mc_f1)+1)/(n_mc_samples+1) # F1 p-value\n",
    "\n",
    "    performance_dict = {\"Accuracy\": (acc, p_acc),\n",
    "                        \"Precision\": (prec, p_prec),\n",
    "                        \"Recall\": (recall, p_recall),\n",
    "                        \"F1\": (f1, p_f1)}\n",
    "    return performance_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOTS = True # Whether to plot Monte-Carlo distributions\n",
    "VARY_P_POS = None # Amount by which the Monte-Carlo predictions should vary from the model's prediction of the proportion of positive predictions. If None, then no variation\n",
    "# TO-DO: If VARY_P_POS is not None, then what distribution should we choose from? Uniform? Gaussian? Beta?\n",
    "\n",
    "# Generate test predictions\n",
    "with torch.no_grad():\n",
    "    output_test = torch.squeeze(model(X_test))\n",
    "    output_test = torch.round(output_test)\n",
    "p_pos_pred = sum(output_test==1)/len(output_test) # Predicted proportion of 1's, use these for Monte-Carlo simulations\n",
    "p_pos_true = sum(y_test==1)/len(y_test) # True proportion of 1's\n",
    "print(f\"Predicted proportion of 1's: {p_pos_pred:.2f}\")\n",
    "print(f\"True proportion of 1's: {p_pos_true:.2f}\")\n",
    "\n",
    "# Generate Monte-Carlo predictions and performance metrics\n",
    "if not MC_SIMULATED:\n",
    "    MC_PREDICTIONS = monte_carlo_bin_pred(len(y_test), int(1e4), p_pos_pred.item(), vary_p_pos=VARY_P_POS)\n",
    "    MC_PERFORMANCE_DICT = monte_carlo_performance(y_test.cpu().numpy(), MC_PREDICTIONS, plots=PLOTS)\n",
    "    MC_SIMULATED = True\n",
    "\n",
    "# Get model's performance metrics, with p-values based on Monte-Carlo simulations\n",
    "performance_dict = get_performance(y_test.cpu().numpy(), output_test.cpu().numpy(), MC_PERFORMANCE_DICT, plots=PLOTS)\n",
    "print(f\"Trained model performance:\")\n",
    "for key, value in performance_dict.items():\n",
    "    print(f\"  {key}: ({100*value[0]:.2f}%, p={value[1]:.5f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Compare to randomly-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()\n",
    "TEST_RANDOM_ACTUAL = False # Allows user to change their mind and not run random model\n",
    "if TEST_RANDOM:\n",
    "\n",
    "    # 1. Train random model for same epochs, learning rate as true model\n",
    "    for run, run_dict in RUNS_DICT.items():\n",
    "        if run in RUNS_DICT_RANDOM:\n",
    "            continue\n",
    "\n",
    "        # Create random labels \n",
    "        shuffler = np.random.permutation(y_train.shape[0])\n",
    "        y_train_random = y_train[shuffler]\n",
    "\n",
    "        optimizer_random = torch.optim.Adam(model_random.parameters(), lr=run_dict[\"Learning rate\"])\n",
    "        loss_func = nn.BCELoss()\n",
    "        mean_train_loss_list_random_run, train_loss_list_random_run, val_acc_list_random_run, pos_pred_list_random_run = train(model_random, optimizer_random, loss_func, X_train, y_train_random, X_val, y_val, run_dict[\"Batch size\"], run_dict[\"Number of epochs\"])\n",
    "        mean_train_loss_list_random.extend(mean_train_loss_list_random_run)\n",
    "        train_loss_list_random.extend(train_loss_list_random_run)\n",
    "        val_acc_list_random.extend(val_acc_list_random_run)\n",
    "        pos_pred_list_random.extend(pos_pred_list_random_run)\n",
    "\n",
    "        RUNS_DICT_RANDOM[run] = RUNS_DICT[run].copy()\n",
    "\n",
    "    ## 2. Plot loss, validation accuracy\n",
    "    mean_train_xrange_random = np.linspace(0, len(train_loss_list_random), len(mean_train_loss_list_random))\n",
    "    plt.figure()\n",
    "    plt.scatter(range(len(train_loss_list_random)), train_loss_list_random, alpha=0.15, c=\"b\", label=\"Raw training loss\")\n",
    "    plt.plot(mean_train_xrange_random, mean_train_loss_list_random, c=\"r\", label=\"Epoch-averaged training loss\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xticks(ticks=np.linspace(0, len(train_loss_list_random), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Random Model: Training Loss vs. Epochs\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(val_acc_list_random)), val_acc_list_random)\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xticks(ticks=np.linspace(0, len(train_loss_list_random), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Random Model: Validation Accuracy vs. Epochs\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(range(len(pos_pred_list_random)), pos_pred_list_random, label=\"Model predictions\")\n",
    "    plt.plot(range(len(pos_pred_list_random)), [1-R_TRAIN for i in pos_pred_list_random], label=\"Training set\")\n",
    "    R_VAL = (sum(y_val)/len(y_val)).item()\n",
    "    plt.plot(range(len(pos_pred_list_random)), [R_VAL for i in pos_pred_list_random], label=\"Validation/Test set\")\n",
    "    plt.ylabel(\"Proportion of predictions that are positive\")\n",
    "    plt.xticks(ticks=np.linspace(0, len(train_loss_list_random), n_epochs_actual+1), labels=[f\"{i}\" for i in range(n_epochs_actual+1)])\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Random Model: Proportion of positive-predictions (on validation-set) vs. Epochs\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Evaluate on test-set, with p-values based on Monte-Carlo simulations\n",
    "\n",
    "    # 3.a. Generate test predictions\n",
    "    with torch.no_grad():\n",
    "        output_test_random = torch.squeeze(model_random(X_test))\n",
    "        output_test_random = torch.round(output_test_random)\n",
    "    p_pos_pred_random = sum(output_test_random==1)/len(output_test_random) # Predicted proportion of 1's, use these for Monte-Carlo simulations\n",
    "    print(f\"Predicted proportion of 1's: {p_pos_pred_random:.2f}\")\n",
    "    print(f\"True proportion of 1's: {p_pos_true:.2f}\")\n",
    "\n",
    "    # 3.b. Generate Monte-Carlo predictions and performance metrics\n",
    "    if not MC_RANDOM_SIMULATED:\n",
    "        MC_PREDICTIONS_RANDOM = monte_carlo_bin_pred(len(y_test), int(1e4), p_pos_pred_random.item(), vary_p_pos=VARY_P_POS)\n",
    "        MC_PERFORMANCE_DICT_RANDOM = monte_carlo_performance(y_test.cpu().numpy(), MC_PREDICTIONS_RANDOM, plots=False)\n",
    "        MC_RANDOM_SIMULATED = True\n",
    "\n",
    "    # 3.c. Get model's performance metrics, with p-values based on Monte-Carlo simulations\n",
    "    performance_dict_random = get_performance(y_test.cpu().numpy(), output_test_random.cpu().numpy(), MC_PERFORMANCE_DICT_RANDOM, plots=PLOTS)\n",
    "    print(f\"Random model performance:\")\n",
    "    for key, value in performance_dict_random.items():\n",
    "        print(f\"  {key}: ({100*value[0]:.2f}%, p={value[1]:.5f})\")\n",
    "    \n",
    "    TEST_RANDOM_ACTUAL = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Save model and about.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_runs = [int(i.split(\"_\")[1].replace(\".pt\",\"\")) for i in os.listdir(CNN_DIR) if \".pt\" in i]\n",
    "if len(prev_runs) == 0:\n",
    "    max_run = 0\n",
    "else:\n",
    "    max_run = np.max(np.array(prev_runs))\n",
    "RUN_PATH = os.path.join(CNN_DIR, f\"run_{max_run+1}.pt\")\n",
    "torch.save(model.state_dict(), RUN_PATH)\n",
    "\n",
    "RUN_ABOUT_PATH = RUN_PATH.replace(\".pt\", \".txt\")\n",
    "with open(RUN_ABOUT_PATH, \"w\") as f:\n",
    "    # Random seeds for reproducibility\n",
    "    f.write(f\"Numpy random seed: {NP_RANDOM_SEED}\")\n",
    "    f.write(f\"\\nTorch random seed: {TORCH_RANDOM_SEED}\")\n",
    "    \n",
    "    # Physical constants\n",
    "    f.write(f\"\\n\\nN: {N}\\nJ: {J}\\nFIRST_NONZERO: {FIRST_NONZERO}\") \n",
    "\n",
    "    # Training parameters\n",
    "    f.write(f\"\\n\\nModel: {model._modules}\\nN_SAMPLES: {N_SAMPLES}\\nR_TRAIN: {R_TRAIN}\\nP_POS_TRUE: {p_pos_true}\") # model parameters\n",
    "    f.write(\"\\nRUNS_DICT\")\n",
    "    for k, v in RUNS_DICT.items():\n",
    "        f.write(f\"\\n  {k}: {v}\")\n",
    "\n",
    "    # Model performance\n",
    "    f.write(\"\\n\\nPerformance at \" + f\"VARY_P_POS = {VARY_P_POS}\")\n",
    "    for k, v in performance_dict.items():\n",
    "        f.write(f\"\\n  {k}: ({100*v[0]:.2f}%, p={v[1]})\")\n",
    "\n",
    "    # Random model performance\n",
    "    if TEST_RANDOM and TEST_RANDOM_ACTUAL:\n",
    "        f.write(\"\\n\\nRANDOM MODEL Performance at \" + f\"VARY_P_POS = {VARY_P_POS}\")\n",
    "        for k, v in performance_dict_random.items():\n",
    "            f.write(f\"\\n  {k}: ({100*v[0]:.2f}%, p={v[1]})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys417",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: It's really only worth using this for very large Hamiltonians, large enough that you can't precompute the fermion inner-products and hold them in memory. Otherwise, the benefit of precomputing the inner-products far outweighs anything you'll get from Numba/Cuda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit, njit, prange, cuda, float32, float64, complex64, complex128, types\n",
    "import numba as nb\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Global variables\n",
    "TPB = 16 # threads per block\n",
    "BPG_MULTIPLIER = 1 # blocks per grid multiplier\n",
    "REAL_FLOAT_TYPE = np.float64\n",
    "REAL_COMPLEX_TYPE = np.complex128\n",
    "REAL_INT_TYPE = np.int32\n",
    "LOAD_PYTHON = False#True # Whether to generate Hamiltonians or load them from file (along with the associated coefficients, of course)\n",
    "\n",
    "# Physical constants\n",
    "K = 7 # number of fermionic modes\n",
    "J = 4 # ~\"energy scale\"\n",
    "Q = 4 # order of coupling\n",
    "N = 2*K # number of fermions\n",
    "N_DIM = 2**K # Hilbert space dimensions\n",
    "\n",
    "N_SAMPLES = 10 # number of samples to generate\n",
    "N_JOBS = 20 # number of jobs to run in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Python Hamiltonian as benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define fermionic modes\n",
    "cr = np.array([[0,1],[0,0]])\n",
    "an = np.array([[0,0],[1,0]])\n",
    "id = np.identity(2)\n",
    "id2 = np.array([[-1,0],[0,1]])\n",
    "\n",
    "def c(n):\n",
    "    factors = [id for i in range(n-1)]+[cr]+[id2 for i in range(K-n)]\n",
    "    out = factors[0]\n",
    "    for i in range(1, K):\n",
    "        out = np.kron(out,factors[i])\n",
    "    return out\n",
    "\n",
    "def cd(n):\n",
    "    factors = [id for i in range(n-1)]+[an]+[id2 for i in range(K-n)]\n",
    "    out = factors[0]\n",
    "    for i in range(1, K):\n",
    "        out = np.kron(out,factors[i])\n",
    "    return out\n",
    "\n",
    "#### Define fermions\n",
    "# Compute first N psi's\n",
    "psi_h = np.zeros((N, N_DIM, N_DIM), dtype=np.complex128)\n",
    "for i in range(1,K+1):\n",
    "    psi_h[2*(i-1)] = (c(i)+cd(i))/np.sqrt(2)\n",
    "    psi_h[2*(i-1)+1] = (c(i)-cd(i))*(-1j/np.sqrt(2))\n",
    "\n",
    "## Copy to GPU\n",
    "psi_d = cuda.to_device(psi_h)\n",
    "\n",
    "def H4_python(js): #js being the random coefficients\n",
    "    # Compute Hamiltonian\n",
    "    H = np.zeros((N_DIM, N_DIM), dtype=np.complex128)\n",
    "    for i in range(N-3):\n",
    "        psi_i = psi_h[i]\n",
    "        for j in range(i+1, N-2):\n",
    "            psi_ij=psi_i@psi_h[j]\n",
    "            for k in range(j+1, N-1):\n",
    "                psi_ijk = psi_ij@psi_h[k]\n",
    "                for l in range(k+1, N):\n",
    "                    psi_ijkl = psi_ijk@psi_h[l]\n",
    "                    H += js[i, j, k, l]*psi_ijkl\n",
    "\n",
    "    return H\n",
    "\n",
    "#### Generate random coefficients\n",
    "sigma_j = np.sqrt((J**2)*np.math.factorial(Q-1)/(N**(Q-1)))\n",
    "js_all = [np.random.normal(0, sigma_j, size=tuple([N for i in range(Q)])) for j in range(N_SAMPLES+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run this one if you don't want to wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamiltonian generation, python: 0.0 minutes, 0.32666444778442383 seconds\n"
     ]
    }
   ],
   "source": [
    "# For K=7: 0.2 seconds\n",
    "# For K=10: 62.3 SECONDS\n",
    "if LOAD_PYTHON:\n",
    "    js_test = np.load(os.path.join(\"Excel\", \"Benchmarks\", \"js4_benchmark.npy\"))\n",
    "    H4_python_test = np.load(os.path.join(\"Excel\", \"Benchmarks\", \"H4_python_benchmark.npy\"))\n",
    "else:                   \n",
    "    js_test = js_all[0]\n",
    "\n",
    "    tic = time.time()\n",
    "    H4_python_test = H4_python(js_test)\n",
    "    toc = time.time()\n",
    "    duration = toc-tic\n",
    "    print(f\"Hamiltonian generation, python: {duration//60} minutes, {duration%60} seconds\")\n",
    "\n",
    "    np.save(os.path.join(\"Excel\", \"Benchmarks\", \"js3_benchmark.npy\"), js_test)\n",
    "    np.save(os.path.join(\"Excel\", \"Benchmarks\", \"H3_python_benchmark.npy\"), H4_python_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(os.path.join(\"Excel\", \"Benchmarks\", \"js4_benchmark.npy\"), js_test)\n",
    "#np.save(os.path.join(\"Excel\", \"Benchmarks\", \"H4_python_benchmark.npy\"), H4_python_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define hybrid Python-CUDA Hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: Pretty much the same as the Python Hamiltonian, but use CUDA kernels to parallelize the matrix-multiplications and elementwise-addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H = \\sum_{k>j>i} J_{ijk} \\psi_i \\psi_j \\psi_k$\n",
    "\n",
    "$H[\\alpha, \\beta] = \\sum_{k>j>i} J_{ijk} \\left( \\psi_i \\psi_j \\psi_k[\\alpha, \\beta] \\right)$\n",
    "$ = \\sum_{k>j>i} J_{ijk} \\left( \\psi_{ijk}[\\alpha, \\beta] \\right)$\n",
    "\n",
    "Well, $\\psi_l \\psi_k [\\alpha, \\beta] = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CUDA fast matrix-multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\anaconda3\\envs\\phys417_clone_07_10_23\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ -42. +36.j  -42. +36.j  -42. +36.j  -42. +36.j]\n",
      " [-154.+132.j -154.+132.j -154.+132.j -154.+132.j]\n",
      " [-266.+228.j -266.+228.j -266.+228.j -266.+228.j]\n",
      " [-378.+324.j -378.+324.j -378.+324.j -378.+324.j]]\n",
      "[[ -42. +36.j  -42. +36.j  -42. +36.j  -42. +36.j]\n",
      " [-154.+132.j -154.+132.j -154.+132.j -154.+132.j]\n",
      " [-266.+228.j -266.+228.j -266.+228.j -266.+228.j]\n",
      " [-378.+324.j -378.+324.j -378.+324.j -378.+324.j]]\n"
     ]
    }
   ],
   "source": [
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "# TPB should not be larger than 32 in this example\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/64198479/13697228 by @RobertCrovella\n",
    "    \"\"\"\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=np.complex128)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=np.complex128)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = float32(0.)\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty + i * TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp\n",
    "\n",
    "x_h = np.arange(16).reshape([4, 4]).astype(np.complex128)+2*1j*np.arange(16).reshape([4, 4]).astype(np.complex128)\n",
    "y_h = np.ones([4, 4]).astype(np.complex128)+4*1j*np.ones([4, 4]).astype(np.complex128)\n",
    "z_h = np.zeros([4, 4]).astype(np.complex128)\n",
    "\n",
    "x_d = cuda.to_device(x_h)\n",
    "y_d = cuda.to_device(y_h)\n",
    "z_d = cuda.to_device(z_h)\n",
    "\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = math.ceil(z_h.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(z_h.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "fast_matmul[blockspergrid, threadsperblock](x_d, y_d, z_d)\n",
    "z_h = z_d.copy_to_host()\n",
    "print(z_h)\n",
    "print(x_h @ y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(types.void(float64[:,:], float64[:,:]))\n",
    "def parallel_add(src, dest):\n",
    "    i, j = cuda.grid(2)\n",
    "    if (i<dest.shape[0]) and (j<dest.shape[1]):\n",
    "        cuda.atomic.add(dest, (i,j), src[i,j]) # <-- THIS FUNCTION DISCARDS THE IMAGINARY PART OF COMPLEX NUMBERS. ONLY USE FOR REAL NUMBERS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\anaconda3\\envs\\phys417_clone_07_10_23\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First i: 0.0 minutes, 1.2129058837890625 seconds\n",
      "Estimated time remaining: 0.0 minutes, 14.55487060546875 seconds\n",
      "Duration: 0.0 minutes, 3.1065595149993896 seconds seconds\n"
     ]
    }
   ],
   "source": [
    "def H4_hybrid(H4_d, js_d):\n",
    "    threadsperblock = (TPB, TPB)\n",
    "    blockspergrid_x = math.ceil(N_DIM / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(N_DIM / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    tic = time.time()\n",
    "    for i in range(N-3):\n",
    "        psi_i = psi_d[i]\n",
    "\n",
    "        for j in range(i+1, N-2):\n",
    "            psi_j = psi_d[j]\n",
    "            psi_ij = cuda.device_array(shape=(N_DIM, N_DIM), dtype=np.complex128)\n",
    "            fast_matmul[blockspergrid, threadsperblock](psi_i, psi_j, psi_ij)\n",
    "\n",
    "            for k in range(j+1, N-1):\n",
    "                psi_k = psi_d[k]\n",
    "                psi_ijk = cuda.device_array(shape=(N_DIM, N_DIM), dtype=np.complex128)\n",
    "                fast_matmul[blockspergrid, threadsperblock](psi_ij, psi_k, psi_ijk)\n",
    "\n",
    "                for l in range(k+1, N):\n",
    "                    psi_l = psi_d[l]\n",
    "                    psi_ijkl = cuda.device_array(shape=(N_DIM, N_DIM), dtype=np.complex128)\n",
    "                    fast_matmul[blockspergrid, threadsperblock](psi_ijk, psi_l, psi_ijkl)\n",
    "\n",
    "                    j_ijkl = js_d[i, j, k,l]\n",
    "                    parallel_add[blockspergrid, threadsperblock]((1j**(Q/2))*j_ijkl*psi_ijkl, H4_d)\n",
    "                    #H4_d += (1j**(Q/2))*j_ijkl*psi_ijkl\n",
    "\n",
    "                    #if (i<5) and (j<5) and (k<5) and (l<5):\n",
    "                    #   print(f\"\\nj_ijk: {j_ijk}\")\n",
    "                    #  print(f\"psi_ijk: {np.sum(np.sum(psi_ijk.copy_to_host(), axis=0), axis=0)}\")\n",
    "        \n",
    "        if i==0:\n",
    "            duration = time.time() - tic\n",
    "            exp_dur = duration*(N-2)\n",
    "            print(f\"First i: {duration//60} minutes, {duration%60} seconds\")\n",
    "            print(f\"Estimated time remaining: {exp_dur//60} minutes, {exp_dur%60} seconds\")\n",
    "                \n",
    "js_test_h = js_all[0].astype(np.complex128)\n",
    "js_test_d = cuda.to_device(js_test_h)\n",
    "\n",
    "H4_hybrid_test_h = np.zeros((N_DIM, N_DIM), dtype=np.complex128)\n",
    "H4_hybrid_test_d = cuda.to_device(H4_hybrid_test_h) # Crops host-array copy-overhead warning\n",
    "#H4_hybrid_test_d = cuda.device_array((N_DIM, N_DIM), dtype=np.complex128) # <--- DOESN'T UPDATE ARRAY. NEVER USE.\n",
    "tic = time.time()\n",
    "H4_hybrid(H4_hybrid_test_d, js_test_d)\n",
    "H4_hybrid_test_h = H4_hybrid_test_d.copy_to_host()\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Duration: {duration//60} minutes, {duration%60} seconds seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(H4_python_test, H4_hybrid_test_h))\n",
    "print(np.allclose(H4_python_test.imag, H4_hybrid_test_h.imag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason Numba is discarding the imaginary part of the Hamiltonian..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.29599142+0.j          0.        +0.j          0.        +0.j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.11585132+0.j          0.17995689+0.27755935j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.17995689-0.27755935j  0.23738243+0.j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.        +0.j        ]\n",
      " ...\n",
      " [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  ...  0.23738243+0.j         -0.17995689-0.27755935j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  ... -0.17995689+0.27755935j  0.11585132+0.j\n",
      "   0.        +0.j        ]\n",
      " [ 0.        +0.j          0.        +0.j          0.        +0.j\n",
      "  ...  0.        +0.j          0.        +0.j\n",
      "   0.29599142+0.j        ]]\n"
     ]
    }
   ],
   "source": [
    "print(H4_python_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.29599142+0.j  0.        +0.j  0.        +0.j ...  0.        +0.j\n",
      "   0.        +0.j  0.        +0.j]\n",
      " [ 0.        +0.j -0.11585132+0.j -0.17995689+0.j ...  0.        +0.j\n",
      "   0.        +0.j  0.        +0.j]\n",
      " [ 0.        +0.j -0.17995689+0.j -0.23738243+0.j ...  0.        +0.j\n",
      "   0.        +0.j  0.        +0.j]\n",
      " ...\n",
      " [ 0.        +0.j  0.        +0.j  0.        +0.j ... -0.23738243+0.j\n",
      "   0.17995689+0.j  0.        +0.j]\n",
      " [ 0.        +0.j  0.        +0.j  0.        +0.j ...  0.17995689+0.j\n",
      "  -0.11585132+0.j  0.        +0.j]\n",
      " [ 0.        +0.j  0.        +0.j  0.        +0.j ...  0.        +0.j\n",
      "   0.        +0.j -0.29599142+0.j]]\n"
     ]
    }
   ],
   "source": [
    "print(H4_hybrid_test_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.59198284 0.         0.         ... 0.         0.         0.        ]\n",
      " [0.         0.23170264 0.45450755 ... 0.         0.         0.        ]\n",
      " [0.         0.45450755 0.47476486 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.         0.         0.         ... 0.47476486 0.45450755 0.        ]\n",
      " [0.         0.         0.         ... 0.45450755 0.23170264 0.        ]\n",
      " [0.         0.         0.         ... 0.         0.         0.59198284]]\n"
     ]
    }
   ],
   "source": [
    "diff = np.abs(H4_python_test - H4_hybrid_test_h)\n",
    "print(diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys417_clone_07_10_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

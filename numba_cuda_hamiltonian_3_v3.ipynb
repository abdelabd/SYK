{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis: It's really only worth using this for very large Hamiltonians, large enough that you can't precompute the fermion inner-products and hold them in memory. Otherwise, the benefit of precomputing the inner-products far outweighs anything you'll get from Numba/Cuda. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import time\n",
    "import copy\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "import pandas as pd\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from numba import jit, njit, prange, cuda, float32, float64, complex64, complex128, types\n",
    "import numba as nb\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# Global variables\n",
    "TPB = 16 # threads per block\n",
    "BPG_MULTIPLIER = 1 # blocks per grid multiplier\n",
    "REAL_FLOAT_TYPE = np.float64\n",
    "COMPLEX_FLOAT_TYPE = np.complex128\n",
    "REAL_INT_TYPE = np.int32\n",
    "LOAD_PYTHON = False#True # Whether to generate Hamiltonians or load them from file (along with the associated coefficients, of course)\n",
    "\n",
    "# Physical constants\n",
    "K=10 # number of fermionic modes\n",
    "J=4 # ~\"energy scale\"\n",
    "Q=3 # order of coupling\n",
    "N = 2*K # number of fermions\n",
    "N_DIM = 2**K # Hilbert space dimensions\n",
    "\n",
    "N_SAMPLES = 10 # number of samples to generate\n",
    "N_JOBS = 20 # number of jobs to run in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Define Python Hamiltonian as benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Define fermionic modes\n",
    "cr = np.array([[0,1],[0,0]])\n",
    "an = np.array([[0,0],[1,0]])\n",
    "id = np.identity(2)\n",
    "id2 = np.array([[-1,0],[0,1]])\n",
    "\n",
    "def c(n):\n",
    "    factors = [id for i in range(n-1)]+[cr]+[id2 for i in range(K-n)]\n",
    "    out = factors[0]\n",
    "    for i in range(1, K):\n",
    "        out = np.kron(out,factors[i])\n",
    "    return out\n",
    "\n",
    "def cd(n):\n",
    "    factors = [id for i in range(n-1)]+[an]+[id2 for i in range(K-n)]\n",
    "    out = factors[0]\n",
    "    for i in range(1, K):\n",
    "        out = np.kron(out,factors[i])\n",
    "    return out\n",
    "\n",
    "#### Define fermions\n",
    "# Compute first N psi's\n",
    "psi_h = np.zeros((N, N_DIM, N_DIM), dtype=COMPLEX_FLOAT_TYPE)\n",
    "for i in range(1,K+1):\n",
    "    psi_h[2*(i-1)] = (c(i)+cd(i))/np.sqrt(2)\n",
    "    psi_h[2*(i-1)+1] = (c(i)-cd(i))*(-1j/np.sqrt(2))\n",
    "\n",
    "## Copy to GPU\n",
    "psi_d = cuda.to_device(psi_h)\n",
    "\n",
    "def H3_python(js): #js being the random coefficients\n",
    "    # Compute Hamiltonian\n",
    "    H = np.zeros((N_DIM, N_DIM), dtype=COMPLEX_FLOAT_TYPE)\n",
    "    for i in range(N-2):\n",
    "        psi_i = psi_h[i]\n",
    "        for j in range(i+1, N-1):\n",
    "            psi_ij=psi_i@psi_h[j]\n",
    "            for k in range(j+1, N):\n",
    "                psi_ijk = psi_ij@psi_h[k]\n",
    "                H += (1j**(Q/2))*js[i, j, k]*psi_ijk\n",
    "\n",
    "    return H\n",
    "\n",
    "#### Generate random coefficients\n",
    "sigma_j = np.sqrt((J**2)*np.math.factorial(Q-1)/(N**(Q-1)))\n",
    "js_all = [np.random.normal(0, sigma_j, size=tuple([N for i in range(Q)])) for j in range(N_SAMPLES+1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't run this one if you don't want to wait"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hamiltonian generation, python: 1.0 minutes, 12.107834815979004 seconds\n"
     ]
    }
   ],
   "source": [
    "# For K=7: 0.2 seconds\n",
    "# For K=10: 62.3 SECONDS\n",
    "if LOAD_PYTHON:\n",
    "    js_test = np.load(os.path.join(\"Excel\", \"Benchmarks\", f\"js{Q}_benchmark.npy\"))\n",
    "    H3_python_test = np.load(os.path.join(\"Excel\", \"Benchmarks\", f\"H{Q}_python_benchmark.npy\"))\n",
    "else:                   \n",
    "    js_test = js_all[0]\n",
    "\n",
    "    tic = time.time()\n",
    "    H3_python_test = H3_python(js_test)\n",
    "    toc = time.time()\n",
    "    duration = toc-tic\n",
    "    print(f\"Hamiltonian generation, python: {duration//60} minutes, {duration%60} seconds\")\n",
    "\n",
    "    np.save(os.path.join(\"Excel\", \"Benchmarks\", f\"js{Q}_benchmark.npy\"), js_test)\n",
    "    np.save(os.path.join(\"Excel\", \"Benchmarks\", f\"H{Q}_python_benchmark.npy\"), H3_python_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Define hybrid Python-CUDA Hamiltonian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Method: Pretty much the same as the Python Hamiltonian, but use CUDA kernels to parallelize the matrix-multiplications and elementwise-addition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$H = \\sum_{k>j>i} J_{ijk} \\psi_i \\psi_j \\psi_k$\n",
    "\n",
    "$H[\\alpha, \\beta] = \\sum_{k>j>i} J_{ijk} \\left( \\psi_i \\psi_j \\psi_k[\\alpha, \\beta] \\right)$\n",
    "$ = \\sum_{k>j>i} J_{ijk} \\left( \\psi_{ijk}[\\alpha, \\beta] \\right)$\n",
    "\n",
    "Well, $\\psi_l \\psi_k [\\alpha, \\beta] = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define CUDA fast matrix-multiply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\anaconda3\\envs\\phys417_clone_07_10_23\\lib\\site-packages\\numba\\cuda\\dispatcher.py:488: NumbaPerformanceWarning: \u001b[1mGrid size 1 will likely result in GPU under-utilization due to low occupancy.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. +12.j 0. +12.j 0. +12.j 0. +12.j]\n",
      " [0. +44.j 0. +44.j 0. +44.j 0. +44.j]\n",
      " [0. +76.j 0. +76.j 0. +76.j 0. +76.j]\n",
      " [0.+108.j 0.+108.j 0.+108.j 0.+108.j]]\n",
      "[[0. +12.j 0. +12.j 0. +12.j 0. +12.j]\n",
      " [0. +44.j 0. +44.j 0. +44.j 0. +44.j]\n",
      " [0. +76.j 0. +76.j 0. +76.j 0. +76.j]\n",
      " [0.+108.j 0.+108.j 0.+108.j 0.+108.j]]\n"
     ]
    }
   ],
   "source": [
    "# Controls threads per block and shared memory usage.\n",
    "# The computation will be done on blocks of TPBxTPB elements.\n",
    "# TPB should not be larger than 32 in this example\n",
    "\n",
    "@cuda.jit\n",
    "def fast_matmul(A, B, C):\n",
    "    \"\"\"\n",
    "    Perform matrix multiplication of C = A * B using CUDA shared memory.\n",
    "\n",
    "    Reference: https://stackoverflow.com/a/64198479/13697228 by @RobertCrovella\n",
    "    \"\"\"\n",
    "    # Define an array in the shared memory\n",
    "    # The size and type of the arrays must be known at compile time\n",
    "    sA = cuda.shared.array(shape=(TPB, TPB), dtype=COMPLEX_FLOAT_TYPE)\n",
    "    sB = cuda.shared.array(shape=(TPB, TPB), dtype=COMPLEX_FLOAT_TYPE)\n",
    "\n",
    "    x, y = cuda.grid(2)\n",
    "\n",
    "    tx = cuda.threadIdx.x\n",
    "    ty = cuda.threadIdx.y\n",
    "    bpg = cuda.gridDim.x    # blocks per grid\n",
    "\n",
    "    # Each thread computes one element in the result matrix.\n",
    "    # The dot product is chunked into dot products of TPB-long vectors.\n",
    "    tmp = float32(0.)\n",
    "    for i in range(bpg):\n",
    "        # Preload data into shared memory\n",
    "        sA[ty, tx] = 0\n",
    "        sB[ty, tx] = 0\n",
    "        if y < A.shape[0] and (tx + i * TPB) < A.shape[1]:\n",
    "            sA[ty, tx] = A[y, tx + i * TPB]\n",
    "        if x < B.shape[1] and (ty + i * TPB) < B.shape[0]:\n",
    "            sB[ty, tx] = B[ty + i * TPB, x]\n",
    "\n",
    "        # Wait until all threads finish preloading\n",
    "        cuda.syncthreads()\n",
    "\n",
    "        # Computes partial product on the shared memory\n",
    "        for j in range(TPB):\n",
    "            tmp += sA[ty, j] * sB[j, tx]\n",
    "\n",
    "        # Wait until all threads finish computing\n",
    "        cuda.syncthreads()\n",
    "    if y < C.shape[0] and x < C.shape[1]:\n",
    "        C[y, x] = tmp\n",
    "\n",
    "x_h = np.arange(16).reshape([4, 4]).astype(COMPLEX_FLOAT_TYPE) + 1j*np.arange(16).reshape([4, 4]).astype(COMPLEX_FLOAT_TYPE)\n",
    "y_h = np.ones([4, 4]).astype(COMPLEX_FLOAT_TYPE) +1j*np.ones([4, 4]).astype(COMPLEX_FLOAT_TYPE)\n",
    "z_h = np.zeros([4, 4]).astype(COMPLEX_FLOAT_TYPE)\n",
    "\n",
    "x_d = cuda.to_device(x_h)\n",
    "y_d = cuda.to_device(y_h)\n",
    "z_d = cuda.to_device(z_h)\n",
    "\n",
    "threadsperblock = (TPB, TPB)\n",
    "blockspergrid_x = math.ceil(BPG_MULTIPLIER*z_h.shape[0] / threadsperblock[0])\n",
    "blockspergrid_y = math.ceil(BPG_MULTIPLIER*z_h.shape[1] / threadsperblock[1])\n",
    "blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "fast_matmul[blockspergrid, threadsperblock](x_d, y_d, z_d)\n",
    "z_h = z_d.copy_to_host()\n",
    "print(z_h)\n",
    "print(x_h @ y_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@cuda.jit(types.void(float64[:,:], float64[:,:]))\n",
    "def parallel_add(src, dest): # <-- THIS FUNCTION DOES NOT SUPPORT COMPLEX NUMBERS\n",
    "    i, j = cuda.grid(2)\n",
    "    if (i<dest.shape[0]) and (j<dest.shape[1]):\n",
    "        cuda.atomic.add(dest, (i,j), src[i,j]) # <-- BECAUSE THIS FUNCTION DOES NOT SUPPORT COMPLEX NUMBERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdel\\anaconda3\\envs\\phys417_clone_07_10_23\\lib\\site-packages\\numba\\cuda\\cudadrv\\devicearray.py:885: NumbaPerformanceWarning: \u001b[1mHost array used in CUDA kernel will incur copy overhead to/from device.\u001b[0m\n",
      "  warn(NumbaPerformanceWarning(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duration: 3.0 minutes, 58.22992444038391 seconds\n"
     ]
    }
   ],
   "source": [
    "# For K=7: 0.5 seconds\n",
    "# For K=10 with TPB=8, bpg_multiplier=1: 3 minutes, 13 seconds\n",
    "# For K=10 with TPB=16, bpg_multiplier=1: 3 minutes, 13 seconds\n",
    "# For K=10 with TPB=32, bpg_multiplier=1: 3 minutes 22 seconds... wtf?\n",
    "# For K=10 with TPB=64, bpg_multiplier=1:\n",
    "def H3_hybrid(H3_real_d, H3_imag_d, js_d):\n",
    "    threadsperblock = (TPB, TPB)\n",
    "    blockspergrid_x = math.ceil(N_DIM / threadsperblock[0])\n",
    "    blockspergrid_y = math.ceil(N_DIM / threadsperblock[1])\n",
    "    blockspergrid = (blockspergrid_x, blockspergrid_y)\n",
    "\n",
    "    for i in range(N-2):\n",
    "        psi_i = psi_d[i]\n",
    "\n",
    "        for j in range(i+1, N-1):\n",
    "            psi_j = psi_d[j]\n",
    "            psi_ij = cuda.device_array(shape=(N_DIM, N_DIM), dtype=COMPLEX_FLOAT_TYPE)\n",
    "            fast_matmul[blockspergrid, threadsperblock](psi_i, psi_j, psi_ij)\n",
    "\n",
    "            for k in range(j+1, N):\n",
    "                psi_k = psi_d[k]\n",
    "                psi_ijk = cuda.device_array(shape=(N_DIM, N_DIM), dtype=COMPLEX_FLOAT_TYPE)\n",
    "                fast_matmul[blockspergrid, threadsperblock](psi_ij, psi_k, psi_ijk)\n",
    "\n",
    "                j_ijk = js_d[i, j, k]\n",
    "                addendum = (1j**(Q/2))*j_ijk*psi_ijk\n",
    "                parallel_add[blockspergrid, threadsperblock](addendum, H3_real_d)\n",
    "                parallel_add[blockspergrid, threadsperblock](1j*addendum, H3_imag_d)\n",
    "    \n",
    "    H3_real_h = H3_real_d.copy_to_host()\n",
    "    H3_imag_h = H3_imag_d.copy_to_host()\n",
    "    H3_h = H3_real_h - 1j*H3_imag_h\n",
    "    return H3_h\n",
    "\n",
    "\n",
    "# NOTE: cuda.device_array doesn't always work for some reason, doesn't update the defined device-array. Just keeps it as zeros. \n",
    "# For example, if you were to define 'H3_real_hybrid_test_d = cuda.device_array((N_DIM, N_DIM), dtype=np.complex128)' and then run 'H3_hybrid_test_h = H3_hybrid(H3_real_hybrid_test_d, H3_imag_hybrid_test_d, js_test_d)', H3_hybrid_test_h would just be an array of zeros at the end. \n",
    "H3_real_hybrid_test_h = np.zeros((N_DIM, N_DIM), REAL_FLOAT_TYPE)\n",
    "H3_real_hybrid_test_d = cuda.to_device(H3_real_hybrid_test_h) \n",
    "\n",
    "H3_imag_hybrid_test_h = np.zeros((N_DIM, N_DIM), REAL_FLOAT_TYPE)\n",
    "H3_imag_hybrid_test_d = cuda.to_device(H3_imag_hybrid_test_h) \n",
    "\n",
    "js_test_h = js_all[0].astype(COMPLEX_FLOAT_TYPE)\n",
    "js_test_d = cuda.to_device(js_test_h)\n",
    "\n",
    "tic = time.time()\n",
    "H3_hybrid_test_h = H3_hybrid(H3_real_hybrid_test_d, H3_imag_hybrid_test_d, js_test_d)\n",
    "toc = time.time()\n",
    "duration = toc - tic\n",
    "print(f\"Duration: {duration//60} minutes, {duration%60} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumbaPerformanceWarning: Host array used in CUDA kernel will incur copy overhead to/from device.\n",
    "\n",
    "  <-- This comes from 'psi' being called in the CUDA function, instead of 'psi_d'. Not sure how to copy list of scipy.sparse arrays to GPU yet, but can probably fix this in sparse-GPU implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K=7: 1.04 seconds\n",
    "\n",
    "K=10: 205.3 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(np.allclose(H3_python_test, H3_hybrid_test_h))\n",
    "print(np.allclose(H3_python_test.real, H3_hybrid_test_h.real))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys417_clone_07_10_23",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
